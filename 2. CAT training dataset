# !pip install catboost shap -q
import os, json, numpy as np
import matplotlib.pyplot as plt
from catboost import CatBoostRegressor, Pool
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from datetime import datetime

DATA_PATH = "/content/drive/MyDrive/anature_revised/ALLCITIES_train_xy_monthly_S10_40pct_ntlmask_TOP100.npz"
RANDOM_SEED = 42
MODEL_OUTDIR = "/content/"

os.makedirs(MODEL_OUTDIR, exist_ok=True)

# ============== 1. Load Data ==============
npz = np.load(DATA_PATH, allow_pickle=True)
X_raw = npz["X"].astype(np.float32)
y = npz["y"].astype(np.float32)
channels_raw = npz["channels"].astype(object).tolist()

print(f"Loaded data: X={X_raw.shape}, y={y.shape}, features={len(channels_raw)}")

# ============== 2. Feature Selection ==============
drop_list = {"ERA5LMAIN.T2M", "ERA5LMAIN.STL1", "ERA5LMAIN.TD", 'META.CITY_ID'}
keep_idx, dropped = [], []

for i, ch in enumerate(channels_raw):
    if ch.upper().startswith("LST."): 
        dropped.append(ch)
    elif ch in drop_list:
        dropped.append(ch)
    else:
        keep_idx.append(i)

X = X_raw[:, keep_idx]
channels = [channels_raw[i] for i in keep_idx]
print(f"Kept {len(channels)} features: {channels}")
print(f"Dropped {len(dropped)} features: {dropped}")

mask = np.isfinite(y)
X, y = X[mask], y[mask]

# ============== 3. Partitioning the Dataset ==============
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=RANDOM_SEED)
train_pool = Pool(X_train, y_train, feature_names=channels)
val_pool = Pool(X_val, y_val, feature_names=channels)

# ============== 4. Training the Model  ==============
try:
    model = CatBoostRegressor(
        iterations=10000,
        depth=8,
        learning_rate=0.1,
        l2_leaf_reg=3.0,
        loss_function="RMSE",
        eval_metric="RMSE",
        random_seed=RANDOM_SEED,
        task_type="GPU",
        devices="0",
        od_type="Iter",
        od_wait=200,
        verbose=200
    )
    use_gpu = True
except:
    model = CatBoostRegressor(
        iterations=3000,
        depth=8,
        learning_rate=0.03,
        l2_leaf_reg=3.0,
        loss_function="RMSE",
        eval_metric="RMSE",
        random_seed=RANDOM_SEED,
        od_type="Iter",
        od_wait=200,
        verbose=200
    )
    use_gpu = False

print("Using GPU?", use_gpu)
model.fit(train_pool, eval_set=val_pool, use_best_model=True)

# ===== 6) Evaluation =====
def evaluate(m, Xv, yv, name="valid"):
    p = m.predict(Xv)
    rmse = mean_squared_error(yv, p) ** 0.5   
    mae  = mean_absolute_error(yv, p)
    r2   = r2_score(yv, p)
    print(f"[{name}] RMSE={rmse:.4f}  MAE={mae:.4f}  R2={r2:.4f}")
    return {"rmse": float(rmse), "mae": float(mae), "r2": float(r2)}

mtr = evaluate(model, X_tr, y_tr, "train")
mva = evaluate(model, X_va, y_va, "valid")

# ============= 6. Drawing the Importance Map =============
importances = model.get_feature_importance()
sorted_idx = np.argsort(importances)[::-1]
topk = min(15, len(channels))

plt.figure(figsize=(8,6))
plt.barh(np.array(channels)[sorted_idx][:topk][::-1], importances[sorted_idx][:topk][::-1])
plt.title("Feature Importance (Top 15)")
plt.xlabel("Importance")
plt.tight_layout()
plt.show()

# ============== 7. Saving Models and Results ==============
stamp = datetime.now().strftime("%Y%m%d_%H%M%S")
OUT_DIR = os.path.join(MODEL_OUTDIR, f"GZ_model_{stamp}")
os.makedirs(OUT_DIR, exist_ok=True)

model.save_model(os.path.join(OUT_DIR, "catboost_model.cbm"))
np.save(os.path.join(OUT_DIR, "channels.npy"), np.array(channels, dtype=object))
np.save(os.path.join(OUT_DIR, "X_valid.npy"), X_val)
np.save(os.path.join(OUT_DIR, "y_valid.npy"), y_val)
with open(os.path.join(OUT_DIR, "metrics.json"), "w") as f:
    json.dump({"train": mtr, "valid": mva, "use_gpu": use_gpu,
               "dropped": list(map(str, dropped))}, f, indent=2)

print(f"✅ The model and validation data have been saved to: {OUT_DIR}")

If an error occurs during execution, you may try the following:

# !pip install catboost shap -q
import os, json, numpy as np
import matplotlib.pyplot as plt
from catboost import CatBoostRegressor, Pool
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from datetime import datetime


DATA_PATH = "/content/drive/MyDrive/anature_revised/ALLCITIES_train_xy_monthly_S10_40pct_ntlmask_TOP100.npz"
MODEL_OUTDIR = "/content/"

os.makedirs(MODEL_OUTDIR, exist_ok=True)

npz = np.load(DATA_PATH, allow_pickle=True)
X_raw = npz["X"].astype(np.float32)
y = npz["y"].astype(np.float32)
channels_raw = npz["channels"].astype(object).tolist()

print(f"Loaded data: X={X_raw.shape}, y={y.shape}, features={len(channels_raw)}")


drop_list = {"ERA5LMAIN.T2M", "ERA5LMAIN.TD", 'META.CITY_ID','ERA5LMAIN.STL1'}
keep_idx, dropped = [], []

for i, ch in enumerate(channels_raw):
    if ch.upper().startswith("LST."): 
        dropped.append(ch)
    elif ch in drop_list:
        dropped.append(ch)
    else:
        keep_idx.append(i)

X = X_raw[:, keep_idx]
channels = [channels_raw[i] for i in keep_idx]
print(f"Kept {len(channels)} features: {channels}")
print(f"Dropped {len(dropped)} features: {dropped}")


mask = np.isfinite(y)
X, y = X[mask], y[mask]


RANDOM_SEED = 123
X_tr, X_va, y_tr, y_va = train_test_split(X, y, test_size=0.2, random_state=RANDOM_SEED)

train_pool = Pool(X_train, y_train, feature_names=channels)
val_pool = Pool(X_val, y_val, feature_names=channels)

try:
    model = CatBoostRegressor(
        iterations=10000,
        depth=8,
        learning_rate=0.1,
        l2_leaf_reg=3.0,
        loss_function="RMSE",
        eval_metric="RMSE",
        random_seed=RANDOM_SEED,
        task_type="GPU",
        devices="0",
        od_type="Iter",
        od_wait=200,
        verbose=200
    )
    use_gpu = True
except:
    model = CatBoostRegressor(
        iterations=3000,
        depth=8,
        learning_rate=0.03,
        l2_leaf_reg=3.0,
        loss_function="RMSE",
        eval_metric="RMSE",
        random_seed=RANDOM_SEED,
        od_type="Iter",
        od_wait=200,
        verbose=200
    )
    use_gpu = False

print("Using GPU?", use_gpu)
model.fit(train_pool, eval_set=val_pool, use_best_model=True)

def evaluate(m, Xv, yv, name="valid"):
    p = m.predict(Xv)
    rmse = mean_squared_error(yv, p) ** 0.5
    mae  = mean_absolute_error(yv, p)
    r2   = r2_score(yv, p)
    print(f"[{name}] RMSE={rmse:.4f}  MAE={mae:.4f}  R2={r2:.4f}")
    return {"rmse": float(rmse), "mae": float(mae), "r2": float(r2)}

mtr = evaluate(model, X_tr, y_tr, "train")
mva = evaluate(model, X_va, y_va, "valid")

importances = model.get_feature_importance()
sorted_idx = np.argsort(importances)[::-1]
topk = min(15, len(channels))

plt.figure(figsize=(8,6))
plt.barh(np.array(channels)[sorted_idx][:topk][::-1], importances[sorted_idx][:topk][::-1])
plt.title("Feature Importance (Top 15)")
plt.xlabel("Importance")
plt.tight_layout()
plt.show()

stamp = datetime.now().strftime("%Y%m%d_%H%M%S")
OUT_DIR = os.path.join(MODEL_OUTDIR, f"GZ_model_{stamp}")
os.makedirs(OUT_DIR, exist_ok=True)

model.save_model(os.path.join(OUT_DIR, "catboost_model.cbm"))
np.save(os.path.join(OUT_DIR, "channels.npy"), np.array(channels, dtype=object))
np.save(os.path.join(OUT_DIR, "X_valid.npy"), X_val)
np.save(os.path.join(OUT_DIR, "y_valid.npy"), y_val)
with open(os.path.join(OUT_DIR, "metrics.json"), "w") as f:
    json.dump({"train": mtr, "valid": mva, "use_gpu": use_gpu,
               "dropped": list(map(str, dropped))}, f, indent=2)

print(f"✅ The model and validation data have been saved to: {OUT_DIR}")

